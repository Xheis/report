% Encoding: UTF-8
%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Chris Renton at 2013-10-29 11:09:03 +1100 


%% Saved with string encoding Unicode (UTF-8) 



@book{jay1995write,
	Author = {Jay, Ros},
	Date-Added = {2013-10-28 23:54:06 +0000},
	Date-Modified = {2013-10-28 23:54:06 +0000},
	Publisher = {Pitman},
	Title = {How to write proposals and reports that get results},
	Year = {1995}}

@book{troyka1999simon,
	Author = {Troyka, Lynn Quitman and Hesse, Douglas Dean and Strom, Cy},
	Date-Added = {2013-10-28 23:52:53 +0000},
	Date-Modified = {2013-10-28 23:52:53 +0000},
	Publisher = {Prentice Hall},
	Title = {Simon \& Schuster handbook for writers},
	Year = {1999}}

@book{strunk2007elements,
	Author = {Strunk, William},
	Date-Added = {2013-10-28 23:51:47 +0000},
	Date-Modified = {2013-10-28 23:51:47 +0000},
	Publisher = {Penguin},
	Title = {The elements of style},
	Year = {2007}}

@Misc{TFwebsite2,
  author = {{Google Brain}},
  title  = {TensorFlow.org},
  year   = {2019},
  url    = {https://www.tensorflow.org/},
}

@Book{HandsOnMLTextbook,
  title     = {Hands on Machine Learning with Scikit, Keras \& Tensorflow},
  publisher = {O'Reilly},
  year      = {2017},
  author    = {Aurelien Geron},
}

@Misc{convertTF,
  author = {{Google Brain}},
  title  = {Get started with TensorFlow Lite},
  year   = {2019},
  url    = {https://www.tensorflow.org/install/pip},
}

@Misc{TFpythonVer,
  author = {{Google Brain}},
  title  = {Install TensorFlow with pip | TensorFlow},
  year   = {2019},
  url    = {https://www.tensorflow.org/lite/guide/get_started#2_convert_the_model_format},
}

@Misc{scikitlearnpythonVer,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title  = {Release History — scikit-learn 0.21.2 documentation},
  year   = {2019},
  url    = {https://scikit-learn.org/stable/whats_new.html},
}

@Book{Mitchell1997,
  title     = {Machine Learning},
  publisher = {McGraw-Hill Education},
  year      = {1997},
  author    = {Tom M. Mitchell},
  isbn      = {978-0-07-042807-2},
  url       = {https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077?SubscriptionId=AKIAIOBINVZYXZQZ2U3A&tag=chimbori05-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=0070428077},
}

@Article{Vaizman2017,
  author    = {Yonatan Vaizman and Katherine Ellis and Gert Lanckriet},
  title     = {Recognizing Detailed Human Context in the Wild from Smartphones and Smartwatches},
  journal   = {{IEEE} Pervasive Computing},
  year      = {2017},
  volume    = {16},
  number    = {4},
  pages     = {62--74},
  month     = {oct},
  doi       = {10.1109/mprv.2017.3971131},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{James2017,
  title     = {An Introduction to Statistical Learning},
  publisher = {Springer-Verlag GmbH},
  year      = {2017},
  author    = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  isbn      = {1461471370},
  date      = {2017-09-01},
  ean       = {9781461471370},
  url       = {https://www.ebook.de/de/product/20292548/gareth_james_daniela_witten_trevor_hastie_robert_tibshirani_an_introduction_to_statistical_learning.html},
}

@Article{scikit-learn,
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title   = {Scikit-learn: Machine Learning in {P}ython},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  pages   = {2825--2830},
}

@InCollection{lecun2012efficient,
  author    = {LeCun, Yann A and Bottou, Leon and Orr, Genevieve B and Muller, Klaus-Robert},
  title     = {Efficient backprop},
  booktitle = {Neural networks: Tricks of the trade},
  publisher = {Springer},
  year      = {2012},
  pages     = {9--48},
}

@InCollection{Bogo_Gruber2007,
  author    = {Hermann Gruber and Markus Holzer and Oliver Ruepp},
  title     = {Sorting the Slow Way: An Analysis of Perversely Awful Randomized Sorting Algorithms},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  year      = {2007},
  pages     = {183--197},
  doi       = {10.1007/978-3-540-72914-3_17},
}

@Article{Bogo_Holzer2018,
  author    = {Holzer, Markus and Maurer, Jan-Tobias},
  title     = {Selection Via the Bogo-Method - More on the Analysis of Perversely Awful Randomized Algorithms},
  year      = {2018},
  doi       = {10.4230/lipics.fun.2018.23},
  keywords  = {Computer Science, 000 Computer science, knowledge, general works},
  language  = {en},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
}

@Unpublished{Bogo_max,
  author = {Max Sherman},
  title  = {Bogo-sort is Sort of Slow},
  month  = {jun},
  year   = {2013},
}

@Article{NFL_quote,
  author   = {Magdon-Ismail, Malik},
  title    = {No Free Lunch for Noise Prediction},
  journal  = {Neural Computation},
  year     = {2000},
  volume   = {12},
  number   = {3},
  pages    = {547-564},
  abstract = { No-free-lunch theorems have shown that learning algorithms cannot be universally good. We show that no free funch exists for noise prediction as well. We show that when the noise is additive and the prior over target functions is uniform, a prior on the noise distribution cannot be updated, in the Bayesian sense, from any finite data set. We emphasize the importance of a prior over the target function in order to justify superior performance for learning systems. },
  doi      = {10.1162/089976600300015709},
  eprint   = {https://doi.org/10.1162/089976600300015709},
  url      = {https://doi.org/10.1162/089976600300015709},
}

@Article{bergstra2012random,
  author  = {Bergstra, James and Bengio, Yoshua},
  title   = {Random search for hyper-parameter optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {Feb},
  pages   = {281--305},
}

@Book{MauriceHerlihy2012,
  title     = {The Art of Multiprocessor Programming, Revised Reprint},
  publisher = {Elsevier Science},
  year      = {2012},
  author    = {Maurice Herlihy, Nir Shavit},
  date      = {2012-06-25},
  ean       = {9780123977953},
  pagetotal = {536},
  url       = {https://www.ebook.de/de/product/19187472/maurice_herlihy_nir_shavit_the_art_of_multiprocessor_programming_revised_reprint.html},
}

@Article{Snoek2012,
  author      = {Jasper Snoek and Hugo Larochelle and Ryan P. Adams},
  title       = {Practical Bayesian Optimization of Machine Learning Algorithms},
  year        = {2012},
  abstract    = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  date        = {2012-06-13},
  eprint      = {http://arxiv.org/abs/1206.2944v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1206.2944v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Brochu2010,
  author      = {Eric Brochu and Vlad M. Cora and Nando de Freitas},
  title       = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  year        = {2010},
  abstract    = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
  date        = {2010-12-12},
  eprint      = {http://arxiv.org/abs/1012.2599v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1012.2599v1:PDF},
  keywords    = {cs.LG, G.1.6; G.3; I.2.6},
}

@comment{ Burge Hughes Walsh Limited, Systems Engineering: Design and deliver 
           better products and processes, 2018. [Online]. Available: 
           https://www.burgehugheswalsh.co.uk/Systems-Engineering/ }

@Book{limited2018systems,
  title     = {Systems {Engineering}: {Design} and deliver better products and processes},
  publisher = {[Online]},
  year      = {2018},
  author    = {Burge Hughes Walsh Limited},
}

 @comment{ Heittola, T. (2018). Research: Sound Event Detection. Retrieved 
           from http://www.cs.tut.fi/~heittolt/research-sound-event-detection }

@Book{heittola2018research,
  title  = {Research: Sound Event Detection},
  year   = {2018},
  author = {Heittola, T.},
}

 @comment{ DCASE2018. (n.d.). DCASE Challenge2018. Retrieved from 
           http://dcase.community/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection#results }

@Book{2018n,
  title     = {(n},
  publisher = {d.)},
  year      = {2018},
}

 @comment{ DCASE2017 Results. (n.d.). DCASE2017. Retrieved from 
           http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-sound-event-detection-in-real-life-audio#results }

@Book{2017results,
  title     = {Results},
  publisher = {(n. d.). {DCASE}2017},
  year      = {2017},
  url       = {http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-sound-event-detection-in-real-life-audio\#results},
}

 @comment{ Google. (2018). AudioSet. (Google) Retrieved from 
           https://research.google.com/audioset/dataset/car.html }

@Book{Audiosetgoogle2018,
  title     = {AudioSet: An Ontology and Human-labeled dataset for audio events},
  publisher = {Google},
  year      = {2018},
  author    = {Google},
}

 @comment{ DCASE. (2018). Schedule. Retrieved from http://dcase.community/ }

@Book{2018Schedule,
  title     = {(DCASE 2018 Schedule)},
  year      = {2018},
  publisher = {Schedule},
  author    = {DCASE.},
  url       = {http://dcase.community/},
}
http://www.k-jahn.de/files/bibtex.xslhttp://www.k-jahn.de/files/bibtex.xslhttp://www.k-jahn.de/files/bibtex.xsl

@Article{winners,
  author = {JiaKai, Lu},
  title  = {MEAN TEACHER CONVOLUTION SYSTEM FOR DCASE 2018 TASK},
  year   = {2018},
}

http://www.k-jahn.de/files/bibtex.xslhttp://www.k-jahn.de/files/bibtex.xslhttp://www.k-jahn.de/files/bibtex.xslhttp://www.k-jahn.de/files/bibtex.xsl

@Article{Yan16,
  author  = {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
  title   = {Language Modeling with Gated Convolutional Networks},
  journal = {CoRR},
  year    = {2016},
}

@TechReport{Bur15,
  title = {The Systems Thinking Tool Box},
  year  = {2015},
}


@Manual{IEEE15288,
  title        = {IEEE 15288: Systems and software engineering -- System life cycle processes},
  author       = {International Organization for Standardization},
  organization = {International Organization for Standardization},
  edition      = {IEEE 15288},
  year         = {2015},
  note         = {International Organization for Standardization. (n.d.). Systems and software engineering -- System life cycle processes. Retrieved from https://www.iso.org/standard/63711.html},
}

@Book{SysThink2015,
  title     = {The Systems Thinking Tool Box},
  publisher = {[Online]},
  year      = {2015},
  author    = {Burge Hughes Walsh Limited},
}

@TechReport{QueenslandReview,
  author      = {The University of Queensland},
  title       = {Literature review},
  institution = {The University of Queensland},
  year        = {2018},
}

@Misc{ToniWebsite,
  author = {Toni Heittola},
  title  = {Research: Sound Event Detection},
  year   = {2018},
}

@Misc{pixelteardown,
  author = {Scott Havard},
  title  = {Google Pixel XL Teardown},
  month  = oct,
  year   = {2016},
  note   = {https://www.electronicproducts.com/Google\_Pixel-whatsinside\_text-205.aspx},
}

@Misc{androidMotionsensors,
  author = {Google},
  title  = {Sensors Motion},
  year   = {2019},
}

 @comment{ D. A. Huffman, "The Synthesis of Sequential Switching Circuits," J. 
           Franklin Institute, vol. 257, no. 3, pp. 161-190, March 1954. }

@Article{huffman1954the,
  author  = {D. A. Huffman},
  title   = {The Synthesis of Sequential Switching Circuits},
  journal = {J. {Franklin} {Institute}, vol. 257, no},
  year    = {1954},
  volume  = {257},
  number  = {3},
  pages   = {161--190},
  month   = mar,
}

@Misc{,
}

@Misc{scikitlearnConMatrix,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title  = {Confusion matrix — scikit-learn 0.21.2 documentation},
  year   = {2019},
}

@Misc{TF_ROC,
  author = {{Google Brain}},
  title  = {Classification: ROC Curve and AUC | Machine Learning Crash Course | Google Developers},
  year   = {2019},
  url    = {https://www.tensorflow.org/install/pip},
}

@Article{Phan2015,
  author    = {Huy Phan and Marco Maas and Radoslaw Mazur and Alfred Mertins},
  title     = {Random Regression Forests for Acoustic Event Detection and Classification},
  journal   = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  year      = {2015},
  volume    = {23},
  number    = {1},
  pages     = {20--31},
  month     = {jan},
  doi       = {10.1109/taslp.2014.2367814},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Phan2016,
  author       = {Huy Phan and Philipp Koch and Fabrice Katzberg and Marco Maass and Radoslaw Mazur and Ian McLoughlin and Alfred Mertins},
  title        = {What Makes Audio Event Detection Harder than Classification?},
  abstract     = {There is a common observation that audio event classification is easier to deal with than detection. So far, this observation has been accepted as a fact and we lack of a careful analysis. In this paper, we reason the rationale behind this fact and, more importantly, leverage them to benefit the audio event detection task. We present an improved detection pipeline in which a verification step is appended to augment a detection system. This step employs a high-quality event classifier to postprocess the benign event hypotheses outputted by the detection system and reject false alarms. To demonstrate the effectiveness of the proposed pipeline, we implement and pair up different event detectors based on the most common detection schemes and various event classifiers, ranging from the standard bag-of-words model to the state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst dataset show significant improvements to detection performance. More importantly, these improvements are consistent for all detector-classifier combinations.},
  date         = {2016-12-29},
  year         = {2016},
  doi          = {10.23919/EUSIPCO.2017.8081709},
  eprint       = {http://arxiv.org/abs/1612.09089v4},
  eprintclass  = {cs.SD},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1612.09089v4:PDF},
  journaltitle = {Published in Proceedings of the 25th European Signal Processing Conference (EUSIPCO), pp. 2739-2743, 2017},
  keywords     = {cs.SD},
}

@Article{Virtanen2010,
  author    = {Virtanen, Tuomas},
  title     = {Acoustic Event Detection In Real Life Recordings},
  year      = {2010},
  doi       = {10.5281/zenodo.42126},
  publisher = {Zenodo},
}

@Article{Tseng2017,
  author      = {Shao-Yen Tseng and Juncheng Li and Yun Wang and Joseph Szurley and Florian Metze and Samarjit Das},
  title       = {Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection},
  abstract    = {State-of-the-art audio event detection (AED) systems rely on supervised learning using strongly labeled data. However, this dependence severely limits scalability to large-scale datasets where fine resolution annotations are too expensive to obtain. In this paper, we propose a small-footprint multiple instance learning (MIL) framework for multi-class AED using weakly annotated labels. The proposed MIL framework uses audio embeddings extracted from a pre-trained convolutional neural network as input features. We show that by using audio embeddings the MIL framework can be implemented using a simple DNN with performance comparable to recurrent neural networks. We evaluate our approach by training an audio tagging system using a subset of AudioSet, which is a large collection of weakly labeled YouTube video excerpts. Combined with a late-fusion approach, we improve the F1 score of a baseline audio tagging system by 17%. We show that audio embeddings extracted by the convolutional neural networks significantly boost the performance of all MIL models. This framework reduces the model complexity of the AED system and is suitable for applications where computational resources are limited.},
  date        = {2017-12-27},
  year        = {2017},
  eprint      = {http://arxiv.org/abs/1712.09673v2},
  eprintclass = {cs.SD},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1712.09673v2:PDF},
  keywords    = {cs.SD, eess.AS},
}

@Book{PsychologicalDebriefing2012,
  title     = {Psychological Debriefing},
  publisher = {Cambridge University Press},
  year      = {2012},
  isbn      = {0521647002},
  date      = {2012-05-17},
  ean       = {9780521647007},
  pagetotal = {388},
  url       = {https://www.ebook.de/de/product/3766790/psychological_debriefing.html},
}

@TechReport{DebriefAAR,
  author      = {Morrison, John E. ; Meliza, Larry L.},
  title       = {Foundations of the After Action Review Process},
  institution = {INSTITUTE FOR DEFENSE ANALYSES ALEXANDRIA VA},
  year        = {1999},
}

@PhdThesis{recentAAR,
  author = {Jeffrey Kaliner},
  title  = {WHEN WILL WE EVER LEARN? THE AFTER ACTION REVIEW, LESSONS LEARNED AND THE NEXT STEPS IN TRAINING AND EDUCATING THE HOMELAND SECURITY ENTERPRISE FOR THE 21ST CENTURY},
  school = {NAVAL POSTGRADUATE SCHOOL MONTEREY, CALIFORNIA},
  year   = {2013},
}

@TechReport{TC2520,
  author      = {HEADQUARTERS, Department of the Army},
  title       = {Circular (TC) 25-20: A Leader’s Guide to After Action Reviews},
  institution = {Department of the Army},
  year        = {1993},
}

@Article{johnson2011debriefing,
  author = {Johnson Pivec, Cynthia Renee},
  title  = {Debriefing after simulation: Guidelines for faculty and students},
  year   = {2011},
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@Booklet{GSMA,
  title  = {The Mobile Economy 2019},
  author = {GSM Association},
  year   = {2019},
  url    = {https://www.gsmaintelligence.com/research/?file=b9a6e6202ee1d5f787cfebb95d3639c5&download},
}

@Misc{dcase2019,
  author = {DCASE},
  title  = {Task A: Preliminary Results},
  note   = {http://dcase.community/challenge2018/task-acoustic-scene-classification#evaluation},
}

@InProceedings{Mesaros2018_DCASE,
  author    = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title     = {A multi-device dataset for urban acoustic scene classification},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},
  year      = {2018},
  pages     = {9--13},
  month     = {November},
  abstract  = {This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.},
  keywords  = {Acoustic scene classification, DCASE challenge, public datasets, multi-device data},
  url       = {https://arxiv.org/abs/1807.09840},
}

@Comment{jabref-meta: databaseType:bibtex;}
